{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistics Document\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Logistics Basics\n",
    "\n",
    "---\n",
    "\n",
    "对一般的2分类有：\n",
    "\n",
    "$$p( y_i=1 | x_i) = \\frac{1}{1+e^{-\\beta^{T} x_i}}$$\n",
    "\n",
    "$$ = \\frac{e^{\\beta^{T} x_i}}{1+e^{\\beta^{T} x_i}} \\tag{1.1}$$\n",
    "\n",
    "$$p( y_i=0 | x_i) = \\frac{1}{1+e^{\\beta^{T} x_i}} \\tag{1.2}$$\n",
    "\n",
    "合并两个式子为\n",
    "\n",
    "$$p(y_i|x_i) = [p(y_i = 1|x_i)]^{y_i} [p(y_i = 0|x_i)]^{1-y_i} \\tag{1.3}$$\n",
    "\n",
    "目标方程是最大化似然函数，似然函数为：\n",
    "\n",
    "$$L(\\beta) = \\prod^{n}_{i=1} \\left(\\frac{e^{\\beta^{T} x_i}}{1+e^{\\beta^{T} x_i}}\\right)^{y_i} \\left(\\frac{1}{1+e^{\\beta^{T} x_i}}\\right)^{1-y_i} \\tag{1.4}$$\n",
    "\n",
    "两边取对数有对数似然：\n",
    "\n",
    "$$LL(\\beta) = \\sum^{n}_{i=1} \\left(\n",
    "    y_i \\ln (\\frac{e^{\\beta^{T} x_i}}{1+e^{\\beta^{T} x_i}}) + (1-y_i) \\ln (\\frac{1}{1+e^{\\beta^{T} x_i}}) \n",
    "    \\right)$$\n",
    "    \n",
    "$$LL(\\beta) = \\sum^{n}_{i=1} \\left( \n",
    "    y_i \\beta^{T} x_i - \\ln (1+e^{\\beta^Tx_i})\n",
    "    \\right)$$\n",
    "    \n",
    "$$ = \\sum^{n}_{i=1} \\left( \n",
    "    y_i f(x_i) - \\ln (1+e^{f(x_i)})\n",
    "    \\right)  \\tag{1.5}$$\n",
    "\n",
    "方法一：梯度下降\n",
    "\n",
    "方法二：牛顿法：\n",
    "\n",
    "要求$f(x)=0$的解，由\n",
    "$$f(x_{n+1}) = f(x_n)+f'(x_n)(x_{n+1}-x_n) \\tag{1.6}$$\n",
    "得到有\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} \\tag{1.7}$$\n",
    "\n",
    "对$\\beta$求导有：\n",
    "$$ S(\\beta) = \\frac{\\partial LL(\\beta)}{\\partial \\beta} = \\sum^{n}_{i=1} \\left( \n",
    "    y_i x_i - \\frac{e^{\\beta^{T} x_i}}{1+e^{\\beta^{T} x_i}}x_i\n",
    "    \\right)$$\n",
    "二次求导有，一般称二阶导数的负数为信息矩阵：\n",
    "\n",
    "$$ I(\\beta) = - \\frac{\\partial^2 LL(\\beta)}{\\partial \\beta \\partial \\beta^{T}}$$\n",
    "\n",
    "$$ = \\sum^{n}_{i=1} \\left( x_i x_i^{T} \\frac{e^{\\beta^{T} x_i}}{1+e^{\\beta^{T} x_i}} \\frac{1}{1+e^{\\beta^{T} x_i}}\\right)$$\n",
    "\n",
    "最后更新公式为：\n",
    "\n",
    "$$ \\beta^{(r+1)} = \\beta^{(r)} + \\left( I(\\beta^{(r)})\\right)^{-1} S(\\beta)\n",
    " \\tag{1.8}$$\n",
    "\n",
    "### 矩阵形式\n",
    "\n",
    "* y是n\\*1的列向量\n",
    "* X是n\\*d的数据矩阵，包括截距项\n",
    "* $P^{(r)}$是n\\*1的概率列向量\n",
    "* W是n\\*n的对角矩阵，对角元素ii为$P(x_i)[1-P(x_i)]$\n",
    "\n",
    "有：\n",
    "\n",
    "$$ S(\\beta^{(r)}) = X^T(y-P^{(r)})$$\n",
    "\n",
    "$$I(\\beta^{(r)}) = X^TWX$$\n",
    "\n",
    "因此有更新公式为\n",
    "\n",
    "$$\\beta^{(r+1)} = \\beta^{(r)} + (X^TWX)^{-1}X^T(y-P^{(r)})$$\n",
    "\n",
    "$$ = (X^TWX)^{-1} (X^TWX) \\beta^{(r)}  + (X^TWX)^{-1}X^T(y-P^{(r)})$$\n",
    "\n",
    "$$ = (X^TWX)^{-1} X^TW z$$\n",
    "\n",
    "其中z为 $z = X\\beta^{(r)} + W^{-1}(y-P^{(r)})$ \n",
    "\n",
    "牛顿法和最小二乘法之间有很强的联系。**$\\beta$的更新公式可以化简为关于z的最小二乘的解？**\n",
    "\n",
    "## 2. Multi-logistics Basics\n",
    "---\n",
    "\n",
    "softmax函数，最终目标公式为\n",
    "\n",
    "$$p(y_i = l|x_i) = \\frac{e^{\\beta_l^{T} x_i}}{1+\\sum^{K-1}_{i=1}e^{\\beta_i^{T} x_i}} \\tag{1.9}$$\n",
    "\n",
    "$$p(y_i = K|x_i) = \\frac{1}{1+\\sum^{K-1}_{i=1}e^{\\beta_i^{T} x_i}} \\tag{1.9}$$\n",
    "\n",
    "\n",
    "## 3. L2 Logistics\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4. L2 Kernel Logistics\n",
    "---\n",
    "\n",
    "一般叫做**KLR**，Kernel Logistics Regression\n",
    "\n",
    "和SVM有很强的关联（损失函数）\n",
    "\n",
    "* SVM[一般指的是soft margin SVM]的损失函数是hinge= $(1-y_if(x_i))_+ = max(1-y_if(x_i),0)$。那可以把SVM的目标函数写成如下的**有约束形式**和**无约束形式**\n",
    "\n",
    "$$\\frac{1}{2}w^Tw+C \\sum^N_{i=1} \\xi_i$$\n",
    "$$s.t. y_i(w^Tx_i + b)> 1 - \\xi_i$$\n",
    "$$\\xi_i\\geq0, i =1,2, ...N $$\n",
    "\n",
    "$$\\frac{1}{2}w^Tw+C \\sum^N_{i=1} max(1-y_if(x_i),0)$$\n",
    "\n",
    "注意：这边L2的话，没有考虑**截距项系数的Regularization**\n",
    "\n",
    "这边无约束的式子比较难用\n",
    "\n",
    "* max无法微分\n",
    "* 二次规划没法直接用\n",
    "\n",
    "错误损失曲线可以表示为：\n",
    "\n",
    "![error graph](./graph/error_graph.jpg)\n",
    "\n",
    "\n",
    "## 5. L1 Logistics ([Paper1](http://www.aaai.org/Papers/AAAI/2006/AAAI06-064.pdf),[Paper2](http://www.jmlr.org/papers/volume8/koh07a/koh07a.pdf))\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 6. Import Vector Machine\n",
    "---\n",
    "\n",
    "([Paper 2005](https://papers.nips.cc/paper/2059-kernel-logistic-regression-and-the-import-vector-machine.pdf))\n",
    "\n",
    "本质还是Logistics\n",
    "\n",
    "是建立在KLR（Kernel Logistics Regression）上的SVM。主要目的是为了**克服SVM在多类别分类上的困难，和结果解释的困难**\n",
    "\n",
    "特点：\n",
    "\n",
    "* 它同样具有稀疏性，只使用了training dataset中的一部分，被称为**import vectors**。且比SVM更少。\n",
    "\n",
    "\timport points是让regularized NLL降低最多的points\n",
    "\tsupport points是离边界比较近的points\n",
    "\n",
    "时间复杂度\n",
    "\n",
    "* SVM时间复杂度是$O(N^3)$\n",
    "* KLR时间复杂度是$O(N^3)$\n",
    "* IVM时间复杂度是$O(N^2q^2)$，其中q是import points的数量\n",
    "\n",
    "SVM和KLR和IVM的联系\n",
    "\n",
    "* SVM的损失函数是hinge损失为$(1-yf)_{+}$\n",
    "* KLR的损失函数是negative log-likelihood（NLL）为$\\ln (1+e^{-yf})$\n",
    "    * 两者是非常相似的\n",
    "* IVM是基于KLR的一种改进。用部分点去估计函数f(x)\n",
    "\n",
    "各自的优缺点：\n",
    "\n",
    "* SVM具有计算的稀疏性\n",
    "* KLR可以为每个点提供probability属性，便于解释； 同时可以用来做多类别分类\n",
    "* IVM则是综合了两者\n",
    "\n",
    "KLR的model可以写作\n",
    "\n",
    "$$f(x) = b + \\sum_i \\alpha_iK(x,x_i) \\tag{6.1}$$\n",
    "\n",
    "IVM的sub-model可以写作\n",
    "\n",
    "$$f(x) = b + \\sum_{x_i \\in S} \\alpha_iK(x,x_i) \\tag{6.2}$$\n",
    "\n",
    "其中S是training dataset的一个集合，获取的方法有很多种：\n",
    "\n",
    "1. 把训练集合分成多份，从每份中分别抽取有代表性的$x_i$放入S (Lin 1998)\n",
    "2. 从kernel matrix $[K(x_i,x_j)]_{N \\times N}$ 抽取q列使得这q列可以大致近似整个核矩阵的 Frobenius范数 (Smola 2000)\n",
    "3. 随机抽取q点，使用Nystrom方法来近似核矩阵的特征分解，然后再把分解还原成N\\*N的(Williams 2001)\n",
    "\n",
    "在IVM中，这个S的构造则使用到了$y_i$\n",
    "\n",
    "### KLR\n",
    "\n",
    "一般的KLR的目标函数是：\n",
    "   \n",
    "$$H = - \\sum^{N}_{i=1}\\left[ y_i f(x_i) - \\ln (1 + e^{f(x_i)})\n",
    "\\right] + \\frac{\\lambda}{2}||f||^2_{H} \\tag{6.3}$$\n",
    "\n",
    "用矩阵的形式可以写作：\n",
    "\n",
    "$$H = -y^{T}(K_{a}\\alpha) + 1^{T} \\ln(1+exp(K_{a}\\alpha)) + \\frac{\\lambda}{2} \\alpha^{T}K_{q}\\alpha \\tag{6.4}$$\n",
    "\n",
    "注意：$1^{T}$是一个长度为n的行向量\n",
    "\n",
    "要找到合适的$\\alpha$向量。需要让H对$\\alpha$求导为0，使用牛顿法，最后更新公式可以写作\n",
    "\n",
    "$$\\alpha^{(r+1)} = \\left(K^{T}_aW^{(r)}K_a + \\lambda K_q \\right)^{-1} K^T_aW^{(r)}z \\tag{6.5}$$\n",
    "\n",
    "其中\n",
    "\n",
    "* $W^{(r)}$是N\\*N的对角矩阵，对角元素ii是$p(x_i|\\beta^{(r)})(1-p(x_i|\\beta^{(r)}))$\n",
    "* z向量为 $z = (K_a \\alpha^{(r)}+ W^{-1}(y-p))$\n",
    "* $K_q \\not= K_a$\n",
    "\n",
    "### IVM\n",
    "\n",
    "#### 1. Basic algorithm（基于greedy forward strategy）\n",
    "\n",
    "> * Let $S=\\emptyset,R = {x_1,x_2,...x_N},r = 1$\n",
    "> * 对每个 $x_l \\in R$, 令\n",
    "$$f_l(x) = \\sum_{x_j \\in S \\bigcup{x_l}} \\alpha_j K(x,x_j)$$\n",
    "找到系数向量$\\alpha$来最小化$H$\n",
    "\n",
    "> 在$H$中\n",
    ">  \n",
    "> $K^{(l)}_a = [K(x_i,x_j)]_{N \\times (q+1)}$ where $x_i \\in R$ and $x_j \\in S \\bigcup \\{x_l\\}$ \n",
    "> \n",
    "> $K^{(l)}_q = [K(x_i,x_j)]_{(q+1) \\times (q+1)}$ where $x_i,x_j \\in S \\bigcup \\{x_l\\}$\n",
    "> \n",
    "> * 令本轮的$x_{l^*} = argmin_{x_l \\in R} (H)$\n",
    "> \n",
    "> 更新$S = S \\bigcup {x_{l^*}}, R = R - \\{x_{l^*}\\}, H_r = H(x_{l^*}), r = r+1$\n",
    "> \n",
    "> * 重复步骤 2,3 直到$H_r$收敛\n",
    "\n",
    "\n",
    "#### 2. Revised algorithm\n",
    "\n",
    "因为在第二步中，需要使用牛顿法计算$\\alpha$，当抽取的import points q过大的时候，计算代价比较高。因此需要修改\n",
    "\n",
    "> * (step2\\*) 对每个$x_l \\in R$, 使用公式 (6.5) 计算 $\\alpha$ 基于更新后的 $K_a$ 和 $K_q$. 再计算 $H$。只要一次\n",
    "\n",
    "#### 3. Stopping for adding to $S$\n",
    "\n",
    "在第4步判断$H_r$ 是否收敛\n",
    "\n",
    "> $H_1,H_2...H_r$假设 $H_r$是第r轮的H值。事先给定一个k整数间隔\n",
    "> $\\frac{|H_r - H_{r-k}|}{|H_r|} < p$ 就停止\n",
    "\n",
    "#### 4. selecting the optimal $\\lambda$\n",
    "\n",
    "整合了包括选择集合S的过程有\n",
    "\n",
    "> 1. 从一个比较大的$\\lambda$开始\n",
    "> 2. 选择S的过程，在结束迭代后，计算在tunning dataset上的error rate\n",
    "> 3. 缩小$\\lambda$\n",
    "> 4. 重复2，3步骤，且S初始化为本次的最终结果 $S = {x_{i1},...x_{iq_k}}$\n",
    "\n",
    "#### Multi Classes\n",
    "\n",
    "假设有K类\n",
    "\n",
    "这边目标函数变成\n",
    "\n",
    "$$H = - \\sum^{N}_{i=1}\\left[ y_i^T f(x_i) - \\ln (1 + e^{f_1(x_i)} + e^{f_2(x_i)}+ ... + e^{f_{K-1}(x_i)})\n",
    "\\right] + \\frac{\\lambda}{2}||f||^2_{H} \\tag{6.3}$$\n",
    "\n",
    "注意这边$y_i$是一个长度为$K-1$的列向量\n",
    "\n",
    "矩阵向量形式可以写作\n",
    "\n",
    "$$H = -\\sum^N_{i=1}[y_i^T (K_a(i,)A)^T - ln(1+1^T e^{(K_a(i,)A)^T}) + \\frac{\\lambda}{2} \\sum^{K-1}_{j=1} \\alpha_j^T K_q \\alpha_j]$$\n",
    "\n",
    "其中\n",
    "\n",
    "* $K_a(i,)$是K矩阵的第i行\n",
    "* $A = (\\alpha_1,\\alpha_2...\\alpha_{K-1})$\n",
    "\n",
    "### Code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 7. Incremental Import Vector Machine\n",
    "---\n",
    "\n",
    "([Paper 2012](https://arxiv.org/pdf/1708.05966.pdf))\n",
    "\n",
    "简称为I2VM\n",
    "\n",
    "\n",
    "\n",
    "## 8. Probability SVM\n",
    "---\n",
    "\n",
    "和IVM又不太相同，它无法解决SVM多分类的问题。主要目的是为了**解决结果解释的困难**\n",
    "\n",
    "1. Naive one\n",
    "\t\n",
    "\t把SVM算得的$\\beta$和$\\beta_0$作为Logistics的参数计算概率\n",
    "\t\n",
    "\t[失去了极大似然的好处]\n",
    "\t\n",
    "2. Naive two\n",
    "\n",
    "\t把SVM算得的$\\beta$和$\\beta_0$作为Logistics极大似然的起始点\n",
    "\t\n",
    "\t[失去了SVM的好处]\n",
    "\t\n",
    "正确的做饭应该是把SVM计算点的得分作为输入，放入Logistics模型中训练\n",
    "\n",
    "$ x_{new} = w^T_{SVM}\\Phi(x_i)$\n",
    "\n",
    "则新的模型应该为\n",
    "\n",
    "$$p(y_i=1|x_{new}) = \\frac{e^{Ax_{new}+B}}{1+e^{Ax_{new}+B}} $$\n",
    "\t\n",
    "如果模型正确的话应该会有\n",
    "\n",
    "* $A>0$\n",
    "* $B \\approx 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
